{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioAI_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtedder/AudioAI-Project/blob/master/notebooks/AudioAI_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMpf0l82p4Cl",
        "colab_type": "text"
      },
      "source": [
        "#Vinyl Audio QC AI/ML Model Pipeline\n",
        "\n",
        "This notebook prepare the data, feature extraction and build a ML model to detect quality control defects in Vinyl records The goal is detect the following audio QC metrics\n",
        "\n",
        "*   Skips\n",
        "*  Jumps\n",
        "*   Sticks\n",
        "*   Intrusive background noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQrWpzB952LC",
        "colab_type": "text"
      },
      "source": [
        "Reference Links:\n",
        "\n",
        "* [Podcast talk with Librosa creater Brian McFee](https://twimlai.com/twiml-talk-263-librosa-audio-and-music-processing-in-python-with-brian-mcfee/)\n",
        "* [audio-analysis](https://www.ntirawen.com/2018/12/audio-analysis-using-deep-learning.html)\n",
        "* [tensorflow-sound-classification](https://www.iotforall.com/tensorflow-sound-classification-machine-learning-applications/)\n",
        "* [Audio-content-representations](https://www.researchgate.net/figure/Audio-content-representations-On-the-top-a-digital-audio-signal-is-illustrated-with-its_fig2_319700841)\n",
        "* [Music Feature Extraction in Python](https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d)\n",
        "* [Spectrogram, Cepstrum and Mel-Frequency](https://archive.org/details/SpectrogramCepstrumAndMel-frequency_636522)\n",
        "* [Audio Voice Processing Deep Learning](https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/)\n",
        "* [Music Classification](https://medium.com/@sdoshi579/classification-of-music-into-different-genres-using-keras-82ab5339efe0)\n",
        "\n",
        "Audio Software Libraries\n",
        "* [Librosa](https://pypi.org/project/librosa/)\n",
        "* [PyAudio](https://pypi.org/project/PyAudio/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_-J1gNip1Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uncommet this code if you need colboratory tensorflow version to match the versions available in google Cloud ML\n",
        "#!pip install tensorflow==1.12.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7bOasUyqOjD",
        "colab_type": "text"
      },
      "source": [
        "##1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFx9RbZx6Jcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Constants\n",
        "ANNOTATIONS_DIR = \"annotations\"\n",
        "DATA_DIR = \"data\"\n",
        "RAW_DATA_DIR = \"raw_sound_files\"\n",
        "DATA_CSV_FILE = \"data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyxw0CiL6dKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create required directories\n",
        "!mkdir annotations\n",
        "!mkdir data\n",
        "!mkdir raw_sound_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUdZIMVSRLx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get annotations json files from repo storage\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/01Label.json\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/02Label.json\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/03Label.json\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/04Label.json\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/05Label.json\n",
        "!wget -P annotations/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/annotations/06Label.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPbO6QOq2ypy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get raw sound file clips from repo storage\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/01Label.wav\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/02Label.wav\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/03Label.wav\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/04Label.wav\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/05Label.wav\n",
        "!wget -P raw_sound_files/ https://raw.githubusercontent.com/mtedder/AudioAI-Project/master/raw_sound_files/06Label.wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn_oVBVP6JTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ref: https://librosa.github.io/librosa/index.html\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd # data processing, json file I/O (e.g. pd.read_json)\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, Dense, Input, SimpleRNN, TimeDistributed, Flatten, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Import the `pyplot` module\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siCAAuAxZKRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python --version\n",
        "# print(\"Tensorflow version\" + tf.__version__)\n",
        "# import keras;\n",
        "# print(keras.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSazlWfeqXWp",
        "colab_type": "text"
      },
      "source": [
        "##2. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ucnLKIS1rsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd annotations\n",
        "# !rm -rf annotations/.ipynb_checkpoints\n",
        "# !ls -al\n",
        "# %cd ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM5gbYVqO897",
        "colab_type": "text"
      },
      "source": [
        "###Read annotation JSON files & Extract features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQFm-iOUoBMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = []\n",
        "for filename in os.listdir(f'./' + ANNOTATIONS_DIR + '/'):\n",
        "#   print(filename)\n",
        "  dataframe = pd.read_json(ANNOTATIONS_DIR + '/' + filename)\n",
        "  for index, row in dataframe.iterrows():\n",
        "      label = row['labels']    \n",
        "      y, sr = librosa.load(row['path'], mono=True, sr=22050, duration=2.97)\n",
        "      S = librosa.feature.melspectrogram(y, sr=sr)\n",
        "      x = np.array(S.reshape(128, 128, 1))\n",
        "      dataset.append((x,label))    \n",
        "\n",
        "dataset = np.array(dataset)\n",
        "#split into input and labels arrays\n",
        "X, Y = zip(*dataset)\n",
        "x_test = np.array(X)\n",
        "Y_test = np.array(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5W1HVH2pC06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace this code when more data is available\n",
        "# Create batch of sequences for training input - repeated dummy input for testing pipeline\n",
        "x_temp_test= []\n",
        "x_temp_test.append(x_test)\n",
        "x_temp_test.append(x_test)\n",
        "x_temp_test.append(x_test)\n",
        "x_temp_test.append(x_test)\n",
        "x_temp_test.append(x_test)\n",
        "x_temp_test.append(x_test)\n",
        "X_test = np.array(x_temp_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-Sof0kMAANW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize data?\n",
        "##\n",
        "##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt5tEN4eNPvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create test and training datasets\n",
        "#Return a random sample of items from an axis of object - http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html#pandas.DataFrame.sample\n",
        "# train_dataset = dataset.sample(frac=0.5,random_state=0)\n",
        "\n",
        "# train_labels = train_dataset.pop('labels')\n",
        "\n",
        "#Drop specified labels from rows or columns with the given indecies (index)- http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html#pandas.DataFrame.drop\n",
        "# test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "#get labels from the features test dataset\n",
        "# test_labels = test_dataset.pop('labels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kguF3OQZ-2zg",
        "colab_type": "text"
      },
      "source": [
        "**One-Hot Encode the labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTiIH4WJ5Luc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(Y_test)\n",
        "print(integer_encoded)\n",
        "\n",
        "Y_test = np.array(keras.utils.to_categorical(integer_encoded, 4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y3hZYBmkh-L",
        "colab_type": "text"
      },
      "source": [
        "##Feature Engineering\n",
        "[librosa ref functions](https://github.com/mtedder/AudioAI-Project/blob/master/notebooks/audioai_playground.ipynb)\n",
        "\n",
        "[audio ai isolating vocals](https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eS3YAjHmRt9",
        "colab_type": "text"
      },
      "source": [
        "###Load File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKB18bWRmUtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a 30 second sample of the desired audio file\n",
        "FILE_NAME = \"01Label.wav\"\n",
        "FILE_PATH = \"raw_sound_files/\"\n",
        "\n",
        "y, sr = librosa.load(FILE_PATH+FILE_NAME, mono=True, sr=22050, duration=2.97)#default sample rate 22050Hz & duration affects t axis bin size in spectrograms\n",
        "# y, sr = librosa.load(FILE_PATH, mono=True, sr=16384, duration=2.97)#down sample to 16384Hz & duration  affects t axis bin size in spectrograms\n",
        "# y, sr = librosa.load(FILE_PATH, mono=True, sr=256, duration=2.97)#down sample to 256Hz & duration  affects t axis bin size in spectrograms\n",
        "\n",
        "\n",
        "print(y.shape)\n",
        "print(sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbPgCGRul4v1",
        "colab_type": "text"
      },
      "source": [
        "###STFT Plot\n",
        "\n",
        "Short-time Fourier transform (STFT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dfJ_-Dpl-c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = np.abs(librosa.stft(y))\n",
        "db = librosa.amplitude_to_db(D,ref=np.max)\n",
        "# Display a spectrogram\n",
        "# Make a new figure\n",
        "plt.figure(figsize=(24,8))\n",
        "\n",
        "librosa.display.specshow(db,y_axis='log', x_axis='time')\n",
        "plt.title('Power spectrogram')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOwJqmekkv_U",
        "colab_type": "text"
      },
      "source": [
        "###Mel spectrogram Plot\n",
        "[Librosa demo](https://nbviewer.jupyter.org/github/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5xq9Jczkrut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Mel spectrogram\n",
        "# Ref: http://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html#librosa-feature-melspectrogram\n",
        "\n",
        "# Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
        "# S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
        "S = librosa.feature.melspectrogram(y, sr=sr)\n",
        "\n",
        "# Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
        "log_S = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "# Make a new figure\n",
        "plt.figure(figsize=(24,8))\n",
        "\n",
        "# Display the spectrogram on a mel scale\n",
        "# sample rate and hop length parameters are used to render the time axis\n",
        "# librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n",
        "\n",
        "# Put a descriptive title on the plot\n",
        "plt.title('mel power spectrogram')\n",
        "\n",
        "# draw a color bar\n",
        "plt.colorbar(format='%+02.0f dB')\n",
        "\n",
        "# Make the figure layout compact\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LCtj-wKoLn",
        "colab_type": "text"
      },
      "source": [
        "###Extract chroma_stft, rmse, spec_cent, spec_bw, rolloff & mfcc and store in csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_FLwXk2UGyLo",
        "colab": {}
      },
      "source": [
        "#Extract Audio features from the sound file listed in the data dataframe and create data.csv file\n",
        "# Ref: https://medium.com/@sdoshi579/classification-of-music-into-different-genres-using-keras-82ab5339efe0\n",
        "# for index, row in data.iterrows():\n",
        "#     Get raw audio filename from dataframe\n",
        "#     label = row['labels']\n",
        "# Strip file extension from file name\n",
        "json_file = FILE_NAME[:FILE_NAME.index(\".\")]\n",
        "#     y, sr = librosa.load(row['path'], mono=True, sr=None, duration=30)\n",
        "chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "rmse = librosa.feature.rmse(y=y)\n",
        "spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "zcr = librosa.feature.zero_crossing_rate(y)\n",
        "mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "to_append = f'{json_file} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'\n",
        "#     Add mfcc values\n",
        "for e in mfcc:\n",
        "  to_append += f' {np.mean(e)}'\n",
        "to_append += f' {label}'\n",
        "#Save features and labels to csv file\n",
        "file = open(DATA_DIR + '/' + DATA_CSV_FILE, 'a', newline='')\n",
        "with file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(to_append.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXzZvxjBqdgr",
        "colab_type": "text"
      },
      "source": [
        "##3. Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaOsWN5LqlEh",
        "colab_type": "text"
      },
      "source": [
        "##4. Build Model\n",
        "\n",
        "[Based on this paper](https://www.researchgate.net/publication/319700841_A_Tutorial_on_Deep_Learning_for_Music_Information_Retrieval)\n",
        "\n",
        "Choi, Keunwoo & Fazekas, György & Cho, Kyunghyun & Sandler, Mark. (2017). A Tutorial on Deep Learning for Music Information Retrieval. \n",
        "\n",
        "Model structure architecture form reference section 5.7.2 : CRNN: c2 -p2 -c2 -p2 -r1 -r2 -d1\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "The input to every LSTM layer must be three-dimensional.\n",
        "\n",
        "The three dimensions of this input are:\n",
        "\n",
        "* Samples. One sequence is one sample. A batch is comprised of one or more samples.\n",
        "* Time Steps. One time step is one point of observation in the sample.\n",
        "* Features. One feature is one observation at a time step.\n",
        "\n",
        "\n",
        "[Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)\n",
        "\n",
        "[CNN LSTM Model](https://machinelearningmastery.com/cnn-long-short-term-memory-networks/)\n",
        "\n",
        "[CNN+LSTM ](https://towardsdatascience.com/get-started-with-using-cnn-lstm-for-forecasting-6f0f4dde5826)\n",
        "\n",
        "[Time Series Analysis with LSTM using Python's Keras Library](https://stackabuse.com/time-series-analysis-with-lstm-using-pythons-keras-library/)\n",
        "\n",
        "[How to Reshape Input Data for Long Short-Term Memory Networks in Keras](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/)\n",
        "\n",
        "[Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)\n",
        "\n",
        "[understanding lstm](https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47)\n",
        "\n",
        "[audio ai isolating vocals](https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785)\n",
        "\n",
        "[audio-classifier-convNet notebook](https://github.com/ajhalthor/audio-classifier-convNet/blob/master/env_sound_discrimination.ipynb)\n",
        "\n",
        "*Videos**\n",
        "\n",
        "[Sound play with Convolution Neural Networks](https://youtu.be/GNza2ncnMfA)\n",
        "\n",
        "[Convolution Neural Networks - EXPLAINED](https://youtu.be/m8pOnJxOcqY)\n",
        "\n",
        "[A friendly introduction to Convolutional Neural Networks and Image Recognition](https://youtu.be/2-Ol7ZB0MmU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHg_oeJFCl_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Model\n",
        "#Model - groups layers into an object with training and inference features.\n",
        "\n",
        "\n",
        "input_shape = (None, 128,128,1)#(None, 128,128,1)#x.shape#x_test.shape#X_test.shape\n",
        "print(input_shape)\n",
        "\n",
        "#Sequential - Linear stack of layers.\n",
        "model = Sequential()\n",
        "\n",
        "# Layer 1 - c2 2D convolution NN & 2D pooling layer\n",
        "# Wrip in TimeDistributed layer to feed the lstm\n",
        "# Ref:https://keras.io/layers/convolutional/\n",
        "# TimeDistributed - https://keras.io/layers/wrappers/\n",
        "model.add(TimeDistributed(Conv2D(24, (5, 5), strides=(1, 1), padding='same', name='conv1', activation='relu'), name='input_layer', input_shape=input_shape))\n",
        "\n",
        "# p1 2D pooling layer\n",
        "# Ref:https://keras.io/layers/pooling/\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size = (4,2), strides=(4,2),name='pool1'), name='layer2'))\n",
        "\n",
        "# Layer 2 - c2 2D convolution NN & 2D pooling layer\n",
        "# Ref:https://keras.io/layers/convolutional/\n",
        "model.add(TimeDistributed(Conv2D(48, (5, 5), strides=(1, 1), padding='valid', name='conv2', activation='relu'), name='layer3'))\n",
        "\n",
        "# p2 2D pooling layer\n",
        "# Ref:https://keras.io/layers/pooling/\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size = (4,2), strides=(4,2),name='pool2'), name='layer4'))\n",
        "\n",
        "# Prepare output from previous MaxPooling2D to input into lstm\n",
        "model.add(TimeDistributed(Flatten(), name='layer5'))\n",
        "\n",
        "# Layer 3 - r1 1D recurrent NN\n",
        "# Ref: https://keras.io/layers/recurrent/\n",
        "model.add(LSTM(50, name='layer6', activation='relu'))\n",
        "\n",
        "# Layer 4 - r2 2D recurrent NN\n",
        "# Ref: https://keras.io/layers/recurrent/\n",
        "# model.add(LSTM(50, name='lstm2', activation='relu'))\n",
        "\n",
        "# Layer 5 - begin vanilla feed forward NN\n",
        "model.add(Dense(64, name='layer7', activation='relu'))#dense hidden layer\n",
        "model.add(Dropout(rate=0.5))\n",
        "\n",
        "# Layer 6 - output class layer\n",
        "model.add(Dense(4, name='output_layer', activation='softmax'))#output layer\n",
        "\n",
        "#Compile Configures the model for training - https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#compile\n",
        "model.compile(\n",
        "\toptimizer=\"Adam\",\n",
        "\tloss=\"categorical_crossentropy\",\n",
        "\tmetrics=['accuracy','categorical_crossentropy'])\n",
        "\n",
        "#print a simple description of the model - https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#summary\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNKuof4pmfG0",
        "colab_type": "text"
      },
      "source": [
        "**Model Sanity Check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2b2K96A5UEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run an inference on the untrained model using dummy data to test model plumbing\n",
        "#Input needs to be 5 dimensional\n",
        "# result = model.predict(X_test.reshape([-1, 6,128,128,1]))#-1 is used to infer one missing length from the other\n",
        "result = model.predict(X_test)#-1 is used to infer one missing length from the other\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spPxtYwrq_Y6",
        "colab_type": "text"
      },
      "source": [
        "##5. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLHTkSB0rO6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Call the model.fit function\n",
        "#Train using fit which Trains the model for a fixed number of epochs (iterations on a dataset) - https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit\n",
        "epoch = 10 ## the higher this number is the more accurate the prediction will be 10000 is a good number to set it at just takes a while to train\n",
        "history = model.fit(X_test, Y_test, batch_size=1, nb_epoch=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0PQzZkUrEyQ",
        "colab_type": "text"
      },
      "source": [
        "##6. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nTyFqn0sE9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize the model's training progress\n",
        "#History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable)\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History\n",
        "hist = pd.DataFrame(history.history)\n",
        "# Add epoch column to hist dataframe\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "hist.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9mzfFrGsKfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Is this model good? Visualize model performance,\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot categorical crossentropy (loss) vs. Epochs\n",
        "plt.figure()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('categorical crossentropy')\n",
        "plt.plot(hist['epoch'], hist['categorical_crossentropy'], label='categorical crossentropy')\n",
        "plt.legend()\n",
        "# plt.ylim([0,1.25])\n",
        "\n",
        "# Plot accuracy vs. Epochs\n",
        "plt.figure()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.plot(hist['epoch'], hist['acc'], label = 'accuracy')\n",
        "plt.legend()\n",
        "\n",
        "\"#Test model using the test data and evaluate\\n\",\n",
        "\"#Returns the loss value & metrics values for the model in test mode - https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#evaluate\\n\",\n",
        "print(model.metrics_names)\n",
        "\n",
        "# evaluate - Returns the loss value & metrics values for the model in test mode.\n",
        "eval = model.evaluate(x=X_test, y=Y_test, verbose=0)\n",
        "print(eval)\n",
        "# print(\"Testing Loss/Error: {:S} Out\".format(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGqTU0qgrK1b",
        "colab_type": "text"
      },
      "source": [
        "##7. Inference – Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhGHluqCspb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run an inference on the untrained model using dummy data to test model plumbing\n",
        "# X_test = X_test.reshape([-1, 6,128,128,1])#-1 is used to infer one missing length from the other\n",
        "result = model.predict(X_test)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXbxFWkara2H",
        "colab_type": "text"
      },
      "source": [
        "#Deploy to ML Cloud Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4lmrdhrj77",
        "colab_type": "text"
      },
      "source": [
        "##8. Prepare Model for Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d5zK2yXrr27",
        "colab_type": "text"
      },
      "source": [
        "##9. Upload model to existing GCS bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgknoLjKrxNd",
        "colab_type": "text"
      },
      "source": [
        "##Upload GOOGLE_APPLICATION_CREDENTIALS (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmBhdx0sShg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upload GOOGLE_APPLICATION_CREDENTIALS json file from local computer and save to this notebook\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnpxrQ32r65B",
        "colab_type": "text"
      },
      "source": [
        "##Set GOOGLE_APPLICATION_CREDENTIALS environment variable (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G1JENivsXNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = #INSERT YOUR CREDENTIALS FILENAME HERE!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c4GZ6VxsNkY",
        "colab_type": "text"
      },
      "source": [
        "##10. Request online prediction from deployed model"
      ]
    }
  ]
}